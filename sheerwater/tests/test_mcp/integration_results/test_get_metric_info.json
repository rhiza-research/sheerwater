{
  "test_name": "test_get_metric_info",
  "timestamp": "2026-02-11T16:00:47.918501",
  "prompt": "Tell me about the MAE metric using sheerwater tools",
  "response": "Here's the information about the **MAE (Mean Absolute Error)** metric from sheerwater:\n\n## MAE - Mean Absolute Error\n\n| Attribute | Value |\n|-----------|-------|\n| **Full Name** | Mean Absolute Error |\n| **Description** | Measures the average magnitude of errors without considering direction |\n| **Formula** | `MAE = (1/n) * sum(\\|forecast - observed\\|)` |\n| **Range** | [0, infinity) |\n| **Direction** | Lower is better |\n| **Interpretation** | Lower values indicate better forecast accuracy. Units match the variable being evaluated |\n\n### Key Properties\n\n- **Type**: Deterministic metric (`prob_type = 'deterministic'`)\n- **Categorical**: No - it's a continuous metric\n- **Sparse**: No - does not induce NaNs\n- **Valid Variables**: All variables (no restrictions)\n- **Statistics Used**: `['mae']`\n\n### How It Works\n\nMAE is one of the simplest error metrics. It calculates the absolute difference between each forecast and observation, then averages these differences. Because it uses absolute values, it:\n\n1. Treats over-predictions and under-predictions equally\n2. Does not penalize large errors more than small ones (unlike RMSE)\n3. Produces results in the same units as the input variable\n\n### Example Usage\n\n```python\nrun_metric(\n    forecast='ecmwf_ifs_er',\n    truth='chirps_v3',\n    metric='mae',\n    region='Kenya'\n)\n```\n\n### Comparison to Related Metrics\n\n| Metric | Sensitivity to Outliers | Units |\n|--------|------------------------|-------|\n| **MAE** | Low | Same as variable |\n| **MSE** | High | Squared units |\n| **RMSE** | High | Same as variable |\n| **Bias** | N/A (can cancel out) | Same as variable |\n\nMAE is often preferred when you want a simple, interpretable error measure that's robust to outliers.\n",
  "success": true
}